
# 4.2.1 算子（Operator）& 层（Layer）

**算子（Operator）** 和 **层（Layer）** 是当前各种通用神经网络模型中，最基础的组成元件。一般来说，算子用于表示模型中的数学运算，而层用于组织模型中的算子。我们通常将单一的数学函数，抽象为一个算子。

需要注意的是，两者皆为 **工程概念** 。

## **算子（Operator）**

**算子（Operator）** 本身仅代表基础运算。因此，既可以是一对一的输入输出，也可以是多对多的输入输出，可以是有状态的或无状态的，也可以是可微的或不可微的。而在使用中，类似 ReLU 等激活函数，或 Dropout 之类的损失函数，都可以被定义为算子，以方便过程中直接使用。

有状态算子在计算输出时，会对前次计算的结果进行 **一定程度的** 抽象记录，从而 **保存以前的状态** 。循环神经网络 (RNN) 中的循环单元（Recurrent Unit）就属于有状态算子。无状态算子在计算输出时不需要记住以前的状态，卷积神经网络 (CNN) 中的卷积算子就属于无状态算子。

可微算子的导数可以计算，这使得它们可以用于训练神经网络。例如，线性算子和非线性算子都是可微的。不可微算子的导数不能计算，这使得它们不能用于训练神经网络，但能够做最终汇总所用。例如，Maxout 算子就是一个不可微算子。

可见，算子本身是灵活的，基本作用等同于一次单一的数学运算，而不在意具体类型。由它构成了 **整个神经网络中最基础的 “加减乘除” 功能** 。

## **层（Layer）**

**层（Layer）** 是由一组算子组成的，神经网络基本组成部分。这些算子共同执行一个特定的任务。例如，**卷积层（Convolution Layer）** 由一组卷积算子组成，这些算子共同执行卷积操作。**池化层（Pooling Layer）** 由一组池化算子组成，这些算子共同执行池化操作。

层按照 **数学特性** ，可以分为 **线性层（Linear Layer）** 或 **非线性层（Nonlinear Layer）** ，两种类型。线性层由一组线性算子组成，这些算子共同执行线性变换。例如，全连接层就是一个线性层。非线性层由一组非线性算子组成，这些算子共同执行非线性变换。例如，卷积层就是一个非线性层。

层按照 **网络特性**，可以分为 **前馈层（Feed Forward Layer）** 或 **循环层（Recurrent Layer）** 。前馈层中的信息只从输入流向输出。循环层中的信息可以从输入流向输出，也可以从输出流向输入。这种分类方式常被使用在 自注意力网络（Transformer） 的层单元中，也可以适当的用来描述其他类型深度神经网络中的层划分。不过，由于如 CNN、RNN 相较于 Transformer 的层级特点相对单一，所以一般不会这么使用。例如，卷积神经网络 (CNN) 中的卷积层就是一个前馈层，循环神经网络 (RNN) 中的循环单元就是一个循环层，不如直接以数学特性表述的准确。

综上，在工程上，算子常常是以最小的 **方法单元（Method Unit）** 而存在，层相当于由一系列算子按照一定的处理顺序，组成的 **任务单元（Task Unit）** 。而模型（Model）则是由一系列层按照既定目标排列组合，形成的 **作业流水线（Process Pipeline）** 。


[ref]: References_4.md