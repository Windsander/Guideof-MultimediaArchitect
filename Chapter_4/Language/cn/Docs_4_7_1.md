
# 4.7.1 卷积神经网络（CNN [Convolutional Neural Network]）

**卷积神经网络（CNN [Convolutional Neural Network]）** ，是对采用 **卷积核（Convolutional Kernel）**，配合 **层叠网格结构** 构成的流水线，来进行特征提取的一类神经网络的统称。该类型最为擅长抽象图片或超二维（指时空性）信息类型的高维特征。

仍以 AlexNet 为例，原工程示意图之前已有展示：

<center>
<figure>
   <img  
      width = "800" height = "260"
      src="../../Pictures/Alexnet.png" alt="">
    <figcaption>
      <p>图 4.7.1-1 完整的 Alexnet 示意图（工程版）</p>
   </figcaption>
</figure>
</center>

我们用它来做一个，基于 MINST 手写字母图像集的，简单字母分类识别模型。
如下所示：

<center>
<figure>
   <img  
      width = "800" height = "400"
      src="../../Pictures/Alexnet_using.png" alt="">
    <figcaption>
      <p>图 4.7.1-2 以 AlexNet 部署的 MINST 字母识别模型</p>
   </figcaption>
</figure>
</center>

可以发现，该模型在层级设计上，前半部分使用到了一系列由多个 **更偏重功能性** 的特殊隐藏层，即卷积层（Conv）、池化层（Pool），以连接构成复杂结构。之后，在经过一个独立的平化层（Flatten Layer）处理多通道数据完毕，才到达我们熟知类似之前介绍的，三层简易结构组成 **多层感知器（MLP）朴素神经网络（Simple Neural Network）** 的部分。

## **CNN 层级分类**

显然，新引入的 **卷积层（Conv）** 、 **池化层（Pool）** 、 **平化层（Flatten Layer）** ，从神经网络结构上划分，**仍然属于隐藏层** 。但所偏重的处理却更为细分。单纯使用结构体系的称谓，已经不能体现其具体作用了。

介于此，我们不得不采用 **功能分类** 方式，细化隐藏层类，来扩展对 CNN 的结构描述能力。

于是，结合原本输入层、输出层的相对准确描述，通常情况，我们能够将 CNN 的层类型分为 6 类。分别是：

- **输入层（Input Layer）** ，处理接收样本的前处理准备工作；
- **卷积层（Conv [Convolutional Layer]）** ，处理卷积核的移动 和 相关数据过滤工作；
- **池化层（Pool [Pooling Layer]）** ，处理压缩/提升参数量的工作；
- **平化层（Flat [Flatten Layer]）** ，处理接收样本的前处理准备工作；
- **全连接层（FC [Fully Connected Layer]）** ，完成神经网络提取后特征的权重迭代部分；
- **输出层（Output Layer）** ，完成最终特征向量结果输出，交由损失函数处理

不过，因为是按照针对处理任务进行的划分，因此，不同的 CNN 模型，在分类上会有或多或少的不同。例如，有些没有平化层，而有些则会多出独立的激活层（专门用于激活函数生效的隐藏层）。所以需要根据实际情况，做些许调整。但分类原则仍然依照上述标准。

为了区别于基础 输入层、输出层、隐藏层 概念，我们在介绍时统一采用 CNN 前缀（如，CNN-Input），表明特殊的分类形式。

## **CNN 输入层（CNN-Input）**

**CNN 输入层（CNN-Input）** 所做的工作和传统神经网络的输入层工作有一定的区分，除了要完成初步激活外，还需要对输入样本数据做一定的 **预处理（Pre-Process）** 工作。此类主要为 **特征工程（Feature Engineering）** 的相关工作，包括但不限于：

- **数据过滤（Data Filtering）** ，初步筛选部分可以直接从样本本身判断出来的无效数据；
- **标准化（Standardization）** ，将数据转为均值 0 且标准差为 1 的分布，无关原分布特征；
- **归一化（Normalization）** ，将样本数据映射到一定范围区间内，放大或缩小范围；
- **中心化（Zero-Centered）** ，将样本数据转为均值 0 但保留原有分布特征；

这里的 **归一化（Normalization）**，指的是将参与训练的数据，归一到 **统一尺度** 下处理。虽然 $$[0,\ 1]$$ 范围因契合概率特征，常作为考虑范围之一，但并不一定都会选在 $$[0,\ 1]$$ 范围。例如我们在音视频中，以 RGB 作为输入数据时，更希望保留 $$[0,\ 255]$$ 的离散范围作为样本 。

除此之外，还有对原有数据的去关联性、离散化、量化转移。因此，输入层的工作，也被经常称为 **数据预处理（Data Preprocessing）** 。这一部分存在相当多的工作，本书在章节末尾的书目推荐中，已列出相关参考推荐，感兴趣可以自行前往了解。

## **CNN 卷积层（CNN-Conv）**

**CNN 卷积层（CNN-Conv）** 主要采用一些滤波器算法，来针对性的提取特征信息。这一过程中使用的滤波器（Filter）即是我们在本书第三章第二节中介绍的一类类型，涵盖了之前所提到的常用滤波手段在内的一系列滤波处理方式。而这也是 CNN 的核心概念之一。

在 CNN 中，一般将 **采用滤波器称为卷积核（Kernel）** 。

<center>
<figure>
   <img  
      width = "600" height = "210"
      src="../../Pictures/CNN_kernel.png" alt="">
    <figcaption>
      <p>图 4.7.1-3 CNN 卷积层的计算过程示例</p>
   </figcaption>
</figure>
</center>

通过卷积操作，不断的从输入样本（CNN 一般是多维数据）中，提取出滤波后的特征。这一过程实际上是对最终被用来作为训练的输出特征向量，所在高维投影信息的一种过滤和逼近。通过对多层 CNN 卷积层的加权训练，来实现简洁观察到样本集代表数据的本质特征。

**由于卷积操作的特点，CNN 最适合被 GPU 加速的运算，即是卷积核运算。**

## **CNN 池化层（CNN-Pool）**

**CNN 池化层（CNN-Pool）** 是除了 CNN 卷积层外的另一种特征提取方式。它本身其实也可归类为一种算子简单的 CNN 卷积层。为了区别，我们把池化层的卷积核，称为池化核。
因此，CNN 池化层具有 CNN 卷积层的所有特点，并一样利于 GPU 化加速。
池化算子根据前一级输入，一般为 $$2 \times 2$$ 或 $$3 \times 3$$ 大小，移动步长为了避免范围覆盖，会取用等核大小步长。 **常见的池化算子（Pooling Operator）主要有两种** ，分别是：

- **最大值池化（Max-Pooling）** ，以池化核内最大值为输出；
- **核均值池化（Avg-Pooling）** ，以池化核内所有值的均值作为输出；

这两类都是 **向下采样（Subsampling）** 过程，效果如下：

<center>
<figure>
   <img  
      width = "400" height = "400"
      src="../../Pictures/CNN_pool.png" alt="">
    <figcaption>
      <p>图 4.7.1-4 CNN 池化层的计算过程示例 <a href="References_4.md">[18]</a></p>
   </figcaption>
</figure>
</center>

除此外，还有各种类型的其它池化算法，例如：混合池化（Mixed Pooling）、线性探测池化（Linear Probing Pooling）[\[19\]][ref] 、向上采样（Upsampling）的全局池化（Global Pooling）等。 **方法不一而足。**

但池化层的目的，始终是对前级输入的一种，引入相对对抗性的校准方式。使得特征的小范围内值得到突出，或用以磨平部分核内数据干扰的手段。

## **CNN 平化层（CNN-Flat）**

**CNN 平化层（CNN-Flat）** ，从字面意义理解，即把输入变换到指定维度大小数据当特殊处理层。它的意义在于，**为传统 MLP 的神经网络部分，提供可供其学习的输入特征** 。

因此，常见的平化层操作，即将前一级输入直接按照顺序延展到指定维度即可。

<center>
<figure>
   <img  
      width = "400" height = "300"
      src="../../Pictures/CNN_flatten.png" alt="">
    <figcaption>
      <p>图 4.7.1-5 CNN 平化层的计算过程示例</p>
   </figcaption>
</figure>
</center>

通常情况，我们会选择输出扁平化到 **一维张量（1-dim Tensor，即  的有 $$n \times 1$$ 个元素的向量）** 。这个过程如上图展示。

## **CNN 全连接层（CNN-FC）& CNN 输出层（CNN-Output）**

**CNN 全连接层（CNN-FC）** & **CNN 输出层（CNN-Output）** ，相比之前几类，和它俩在 MLP 时期的作用基本无变化。

- **CNN 全连接层（CNN-FC）** 相当于前文中三层朴素神经网络里的隐藏层；
- **CNN 输出层（CNN-Output）** 相当于前文中三层朴素神经网络里的输出层；

此处就不再赘述。

需要注意的是，**CNN 输出层（CNN-Output）的输出结果** ，才是我们 **在训练阶段** 中，用来 **交付损失函数计算，并参与优化器迭代权重的部分** 。为区别于其它，有时会被称为模型的 **关键特征向量（Key Vector）** 。

## **CNN 网络结构**

卷积神经网络存在远超 MLP 的层级，带来的变换远非只停留于对层级功能的细化上。在网络结构层面，也逐渐由处理针对任务性质的差异，产生了在 CNN 整体结构内，更为明确的区分。我们一般将位于神经网络内，专项执行单一主要任务的内部子模块，称为 **子网结构（Sub-Network Structure）** ，或简称为 **子网（Subnet）** 。

同样于层级分类情况，对于主要目的不同的 CNN ，其子网结构也不完全相同。

但一般而言，大体可以分为 3 个子网，分别是：

- **特征提取（FE [Feature Extraction]）子网** ，用于提炼原始信息至高级特征；
- **特征选择（FS [Feature Selection]）子网** ，用于将高级特征抽象至最终输出特征向量；
- **结果输出（RO [Result Output]）子网** ，用于输出最终的处理结果；

从分类可见，特征选择子网和特征提取子网，在卷积神经网络中的作用，基本等同于传统机器学习过程中，特征的选择和提取在传统逻辑回归和聚类分析中，所起到的作用一致。但其作用范围是整张网络内，所有的过程中特征。这一点还是有较大维度上的差异的。

在具体实践中，是什么样的情况呢？

我们以 AlexNet 部署物体识别的 CNN 分类模型为例，有：

<center>
<figure>
   <img  
      width = "800" height = "360"
      src="../../Pictures/Alexnet_detail.png" alt="">
    <figcaption>
      <p>图 4.7.1-6 以 AlexNet 部署的 ImageNet 物体识别模型</p>
   </figcaption>
</figure>
</center>

在例子中，3 个子网结构各包含了多个 AlexNet 的不同层级：

- **特征提取子网（FE）** ，包含 输入层、平化层，以及从输入层至平化层间的多个池化层、卷积层，共同组成；
- **特征选择子网（FS）** ，在本例中根据功能也被称为 分类子网（Classification Subnet），包含接收平化层输出的相邻隐藏层至输出层前一级隐藏层。这些隐藏层都是全链接层，以此完成特征向量提炼。
- **结果输出子网（RO）** ，则是在接收 FS 输出后，最终生成特征向量的传统 MLP 组成。例子中采用了 SoftMax 连接函数，完成了对样本的 **概率分布（Probabilistic Distribution）** 归一化向量输出。

需要注意的是，例子中由于是训练好的模型，并没有画出当模型还在训练时，损失函数生效的阶段。不过，在我们经过前几节的讲解后，还是可以判断得到，其生效位置正是在 RO 之后。
训练阶段的 CNN ，正是接收了结果输出子网的特征向量，以此作为迭代的损失函数输入。

那么 CNN 有哪些适合的优势场景呢？

## **CNN 的常见场景**

考虑到 CNN 的特点，其实只要是超过一维的样本数据，都能够以 CNN 来进行相关作业。这也决定了 CNN 具有极强的普适性。

目前上，工业界对 CNN 的运用已经涵盖了：

- 图像分类，如：手势识别、动作识别、人脸识别等；
- 图像分割，如：背景分离、智能抠图、轮廓融合等；
- 语义分割，如：物体分类、车辆检测等；
- 语音识别，如：文本转译、同声传录、情感分析等；

除此外，包括 2016 年名声大噪的 AlphaGo ，也是采用的 CNN 多模型混合架构。足以见得其巨大的发挥空间。虽然 2022 年因 OpenAI 的 ChatGPT 引起 LLM Transformer 浪潮，让 CNN 的热度有所减退，但并不能阻碍它成为目前最好用的模型结构选择之一。

相信未来，我们仍然能够一睹 CNN 回归 LLM 多模态语言大模型的风采。

至此，CNN 的初级概念和网络结构，基本介绍完毕。有了这些知识背景，在了解 CNN 的各种类型网络的设计时，亦能窥得大概。其余就需要仔细钻研论文，以了解全貌了。


[ref]: References_4.md