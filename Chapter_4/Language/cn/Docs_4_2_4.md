
# 4.2.4 特征选择（Feature Selection）

**特征选择（Feature Selection）** 是每个模型启动前最为重要的一环，也是 **特征工程（Feature Engineering）** 方法论所靶向的关键问题之一。

在传统机器学习（ML）中，特征选择是对影响结果较不明显的 **因变量（Independent Variables）**，以一系列处理手段，转换为较为明显的 **关系参数（Related Parameters）**表示，从而发掘出潜藏影响因素。这一过程所产生的单次影响因子参数，构成的一组标量数组，就是 **特征向量（Feature Vector）**。而对全体样本进行相同流程的抽象，得到的特征向量集合，即是 **训练特征集（Training Feature Set）**。

工程上，训练特征集 通常以 **一批次（1 Batch）** 样本计算后，由神经网络输出的当前权重下，输入样本的抽象非零解集构成。这个输出的抽象特征向量数据集，才是正真被我们用来衡量当前迭代结果情况的决定数据。即，**损失函数（Loss Function）作用的部分**。

而特征选择，正是对如何获取满足模型目标的特征和训练特征集的方法论。

常用的特征选择方式，可以分为三大类别：

- **过滤法（Filtered）**，以相关性和扩散性对特征评分，采用阈限法或策略来筛选；
- **包裹法（Wrapped）**，以评分函数或预测效果校验评分，筛选满足条件特征；
- **嵌入法（Embedded）**，以影响权重加强/衰减影响过程，用权重变换决定特征；

采用不同方法获取的训练集，也根据方法的选择情况，被分别称为 **过滤集（Filterings）** 、 **包裹集（Wrappings）** 、 **嵌入集（Embeddings）**。

显然，在深度学习中，被批量使用的特征选择方法，就是嵌入法。

## **嵌入集（Embeddings）**

经由神经网络抽象高维特征的输出向量数据集，被我们称为嵌入特征向量组（Embeddings of Low-Dimesional Features），简称嵌入集（Embeddings）。与特征工程的相关称谓同名，并不矛盾。
它既可以是一组由 $$n \times m$$ 的向量构成的数组，如下 $$n \times m = 8 \times 1$$ 有：

```C
double embeddings[BATCH_SIZE][VECTOR_SIZE] = {
    {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0},
    {4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0}, /* ... */
};
```

也可以是单纯的评估数据，相当于 $$n \times m = 1$$ 的向量组成：

```C
double predictions[BATCH_SIZE] = {
    0.1, 0.8, 0.2, 0.3, 0.5, 0.7, 1.0, 0.9, /* ... */
};
```

即，组成嵌入集的特征向量形式，并没有特殊的要求。但往往需要根据采用的损失函数来决定最终的格式。这一点在实践中非常重要。由于评估数据常用于线性回归，区别起见被称为 **预测集（Predictions）**。

现在，我们基本掌握了深度学习的入门概念。让我们分步来看，一个神经网络的具体细节。


[ref]: References_4.md